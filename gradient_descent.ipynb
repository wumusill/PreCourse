{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79be6e31",
   "metadata": {},
   "source": [
    "# 미분\n",
    "* 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구\n",
    "* 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있다\n",
    "* 증가시키고 싶다면 미분값을 더하고 감소시키고 싶으면 미분값을 뺀다\n",
    "\n",
    "\n",
    "* 미분값을 더하면 경사상승법(gradient ascent)이라 하며 함수의 극대값의 위치를 구할 때 사용\n",
    "* 미분값을 빼면 경사하강법(gradient descent)이라 하며 함수의 극소값의 위치를 구할 때 사용\n",
    "* 극값에 도달하면 움직임을 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b74828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{Poly}{\\left( 2 x + 2, x, domain=\\mathbb{Z} \\right)}$"
      ],
      "text/plain": [
       "Poly(2*x + 2, x, domain='ZZ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sym\n",
    "from sympy.abc import x, y\n",
    "\n",
    "sym.diff(sym.poly(x**2 + 2*x + 3), x)    # 식을 x로 미분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2672659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/gbsdt1191mbf_hk8mhnwwpd80000gn/T/ipykernel_91113/1617311515.py:1: SymPyDeprecationWarning: \n",
      "\n",
      "Mixing Poly with non-polynomial expressions in binary\n",
      "operations is deprecated. Either explicitly convert\n",
      "the non-Poly operand to a Poly with as_poly() or\n",
      "convert the Poly to an Expr with as_expr().\n",
      "\n",
      "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-poly-nonpoly-binary-operations\n",
      "for details.\n",
      "\n",
      "This has been deprecated since SymPy version 1.6. It\n",
      "will be removed in a future version of SymPy.\n",
      "\n",
      "  sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x + 2 y - \\sin{\\left(x + 2 y \\right)}$"
      ],
      "text/plain": [
       "2*x + 2*y - sin(x + 2*y)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a9f98",
   "metadata": {},
   "source": [
    "# 소프트맥스 연산\n",
    "* 소프트맥스 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산\n",
    "* 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측\n",
    "    * 특정 클래스에 속할 확률 계산\n",
    "    \n",
    "    \n",
    "* 추론을 할 때는 one-hot 벡터로 최대값을 가진 주소만 1로 출력하는 연산을 사용해서 softmax를 사용하진 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30b820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],\n",
       "       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],\n",
       "       [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(vec):\n",
    "    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))\n",
    "    numerator = np.sum(denumerator, axis=-1, keepdims=True)\n",
    "    val = denumerator / numerator\n",
    "    return val\n",
    "\n",
    "vec = np.array([[1, 2, 0], [-1, 0, 1], [-10, 0, 10]])\n",
    "softmax(vec)\n",
    "\n",
    "# 벡터를 확률벡터로 변환 [1, 2, 0] -> [0.24, 0.67, 0.09]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2cef28",
   "metadata": {},
   "source": [
    "## 신경망은 선형모델과 활성함수를 합성한 함수\n",
    "## 활성함수란\n",
    "* R위에 정의된 비선형 함수로써 딥러닝에서 매우 중요한 개념\n",
    "* 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없음\n",
    "* 시그모이드 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에선 ReLU 함수를 많이 씀\n",
    "\n",
    "## 여러 층을 쌓아야 하는 이유\n",
    "* 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있음\n",
    "* 그러나 층이 깊을수록 목적함수를 근사하는데 필요한 뉴련(노드)의 숫자가 훨씬 빠르게 줄어들어 좀 더 효율적으로 학습이 가능\n",
    "* 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은 신경망이 되어야 함\n",
    "* but 층이 깊어질수록 학습이 어려워질 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a869f",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
